{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0105EN-SkillsNetwork/labs/Module2/images/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add your code below following the instructions given in the course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My First Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "\n",
    "Jupyter Notebooks originated as “iPython,” originally developed for Python programming.\n",
    "A Jupyter Notebook is a browser-based application that allows you to create and share documents containing code, equations, visualizations, \n",
    "narrative text links, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**List of Data Science languages:**\n",
    "\n",
    "First and foremost, we recommend considering the languages\n",
    "1. Python\n",
    "2. R\n",
    "3. SQL languages. \n",
    "\n",
    "However, other languages such as Scala, Java, C++, and Julia with specific features are also popular. \n",
    "JavaScript, PHP, Go, Ruby, and Visual Basic all have their own unique use cases as well. \n",
    "The language you choose to learn will depend on the things you need to accomplish and the problems you need to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** List of Data Science libraries:**\n",
    "\n",
    "List the scientific computing libraries in Python List the visualization libraries in Python List the high-level machine learning and deep \n",
    "learning libraries.Libraries are a collection of functions and methods that allow you to perform many actions without writing the code.\n",
    "\n",
    "Python libraries such as : Scientific Computing Libraries in Python Visualization Libraries in Python High-Level- Machine Learning and Deep \n",
    "Learning Libraries (High-level means you don’t have to worry about details making studying or improving difficult.) And finally, Deep Learning \n",
    "Libraries in Python, and Libraries used in other languages. Now, scientific computing libraries contain built-in modules providing different \n",
    "functionalities, which you can use directly.\n",
    "\n",
    "1. Pandas offers data structures and tools for effective data cleaning,manipulation, and analysis. It provides tools to work with different types of data. The primary instrument of Pandas is a two-dimensional table\n",
    "consisting of columns and rows, called a Data Frame. Pandas can also provide easy indexing so you can work with your data. \n",
    "2. NumPy libraries are based on arrays and matrices, allowing you to apply mathematical functions to the arrays. \n",
    "Pandas is built on top of NumPy. You use data visualization methods to communicate with others and display meaningful results of an analysis.\n",
    "These libraries enable you to create graphs, charts, and maps. \n",
    "3. The Matplotlib package is the most well-known library for data visualization. They are popular for making graphs and plots,and the graphs\n",
    "are easily customizable. \n",
    "4. Another high-level visualization library is Seaborn. It is based on matplotlib. This library generates heat maps, time series, and violin\n",
    "plots. \n",
    "5. Now, for machine learning, the Scikit-learn library contains tools for statistical modeling, including regression, classification, \n",
    "clustering, and so on. It is built on NumPy, SciPy, and matplotlib. It is simple to get started. In this high-level approach, you define \n",
    "the model and specify the parameter types you want to use.\n",
    "6.For building deep learning models, Keras allows you to build the standard deep learning model. Like Scikit, the high-level interface allows you to build models in a quick,\n",
    "simple manner. It can function using Graphics processing units (GPU) but in many cases, a lower-level environment is necessary for\n",
    "deep learning. \n",
    "7. TensorFlow is a low-level framework used in the large-scale production of deep learning models. It's designed for production and \n",
    "deployment but can be unwieldy for experimentation. \n",
    "8. Pytorch is used for experimentation, making it simple for researchers to test ideas. \n",
    "9. Apache Spark is a general-purpose cluster-computing framework allowing you to process data using compute clusters. \n",
    "The data is processed in parallel in more than one computer simultaneously.\n",
    "10. The Spark library has similar functionality to the following: Pandas, Numpy, and Scikit-learn. Apache Spark data processing jobs can be in:\n",
    "Python R Scala, and SQL There are many Scala libraries.\n",
    "11. Scala is predominately used in data engineering and data science. Let’s discuss some libraries that are complementary to Spark.\n",
    "12. Vegas is a Scala Library for statistical data visualizations. With Vegas, you can work with data files as well as Spark Data Frames.\n",
    "13. For deep learning, you can use big DL. R has built-in functionality for machine learning and data visualization, but there are also\n",
    "complementary libraries. \n",
    "13. ggplot2 is a popular library for data visualization in R. You can also use libraries that allow you to interface\n",
    "with Keras and TensorFlow. And R was a de-facto standard for open-source data science, but now Python will supersede it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data Scince Tools:**\n",
    "\n",
    "The most widely used open-source data management tools are relational databases like MySQL and PostgreSQL. \n",
    "Also, there are NoSQL Databases like MongoDB, Apache CouchDB, and Apache Cassandra. \n",
    "In addition, there are file-based tools like the Hadoop File System or Cloud File systems like Ceph. \n",
    "You also have an elastic search tool that stores text data, including the creation of a search index for \n",
    "fast document retrieval. \n",
    "Now, the task of data integration and transformation in the classic data warehousing world is to Extract, \n",
    "Transform, and Load (ETL). \n",
    "Data scientists often propose Extract, Load, Transform (ELT) as data is dumped somewhere, and the data engineer or data scientist handles \n",
    "the transformation of the data. Another term for this process emerged: Data Refinery and Cleansing. \n",
    "The most widely used open-source data integration and transformation tools are the following: Apache AirFlow, which was created by Airbnb \n",
    "originally. \n",
    "KubeFlow, which allows the execution of data science pipelines on top of Kubernetes. \n",
    "Apache Kafka, which originated from LinkedIn. \n",
    "Apache Nifi, which delivers a very nice visual editor. \n",
    "Apache SparkSQL, lets you use ANSI SQL and scales up to compute clusters of thousands of node\n",
    "NodeRED also brings a visual editor. In addition, NodeRED is so low in resource consumption that it even runs on tiny devices like a Raspberry Pi. \n",
    "Now let’s discuss the most widely used open-source data visualization tools. \n",
    "You must distinguish between programming libraries where you must use code or tools containing a user interface. \n",
    "Pixie Dust is also a library but has a user interface that facilitates plotting in Python. A similar approach uses Hue, which can create \n",
    "visualizations from SQL queries. \n",
    "Whereas Kibana, a data exploration, and visualization web application is limited to Elasticsearch \n",
    "(data provider). And finally, Apache Superset is a data exploration and visualization web application. \n",
    "Model deployment is a crucial step. Once you’ve created a machine learning model capable of predicting some critical aspects of the \n",
    "future, you should make it consumable by other developers and turn it into an API. \n",
    "Apache PredictionIO currently only supports Apache Spark ML models for deployment, but support for all libraries is on the roadmap. \n",
    "Seldon is an interesting product since it supports nearly every framework including, TensorFlow, Apache SparkML, R, and scikit learn. \n",
    "Interestingly, it can run on top of Kubernetes and Redhat OpenShift. Another way to deploy SparkML models is MLeap. \n",
    "Finally, TensorFlow can serve any tensor flow model using the TensorFlow service. \n",
    "It can be an embedded device like a Raspberry Pi or smartphone using TensorFlow lite and deployed to a web browser using TensorFlow dot JS. \n",
    "Model monitoring is an important step as well. Once you’ve deployed a machine learning model, you want to track its prediction performance \n",
    "while new data arrives to maintain outdated models. Some examples are the following: ModelDB is a machine model metadata base \n",
    "where information about the models is stored and queried. It natively supports Apache Spark ML Pipelines and scikit-learn. \n",
    "A generic, multi-purpose tool called Prometheus is widely used as well. Although it is not specifically made for machine learning model \n",
    "monitoring, it is used for this purpose. Model performance is measured by more than accuracy. Model bias against protected groups like \n",
    "gender or race is important as well. The IBM AI Fairness 360 open-source toolkit detects and mitigates bias in machine learning models. \n",
    "These models, especially neural network-based deep learning models, can be subject to adversarial attacks where an attacker tries to \n",
    "mislead the model with manipulated data or by controlling it. The IBM Adversarial Robustness 360 Toolbox detects vulnerability against \n",
    "adversarial attacks and leverages the model to be more robust. Finally, machine learning modes are often considered as a black box applying \n",
    "some magic. Kylo is an open-source data management software platform, with extensive support for data asset management tasks. \n",
    "\n",
    "1. Data management tools are MySQL, PostgreSQL, MongoDB, Apache CouchDB, Apache Cassandra, Hadoop File System, Ceph, and elastic search. \n",
    "2. Data integration and transformation tools are Apache AirFlow, KubeFlow, Apache Kafka, Apache Nifi, Apache SparkSQL, and NodeRED. \n",
    "3. Data Visualization tools are Pixie Dust, Hue, Kibana, and Apache Superset. \n",
    "4. Model deployment tools are Apache PredictionIO, Seldon, Kubernetes, Redhat OpenShift, Mleap, TensorFlow service, TensorFlow lite, and TensorFlow dot JS.\n",
    "5. Model monitoring tools are ModelDB, Prometheus, IBM AI Fairness 360, IBM Adversarial Robustness 360 Toolbox, and IBM AI Explainability 360. \n",
    "6. Code asset management tools are Git, GitHub, GitLab, and Bitbucket. \n",
    "7. Data asset management tools are Apache Atlas, ODPi Egeria, and Kylo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arithmetic expression:**\n",
    "\n",
    "1. Addition = a+b;\n",
    "2. sutraction = a-b;\n",
    "3. Multiplication = a*b;\n",
    "4. Division = a/b;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "A = 10;\n",
    "B = 20;\n",
    "mult = A*B;\n",
    "sum = A+B;\n",
    "print(mult);\n",
    "print(sum);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Minutes :-->  75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour : Minute :-->  1  :  15\n"
     ]
    }
   ],
   "source": [
    "# Python Program to Convert Minute to Hour\n",
    "min = int(input(\"Enter Minutes :--> \"))\n",
    "print(\"Hour : Minute :--> \", int(min/60), \" : \", min % 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List objectives:**\n",
    "\n",
    "    1. About My first assignment.\n",
    "    2. Introduction to Jupyter Notebook.\n",
    "    3. List of Data Science languages.\n",
    "    4. List of Data Science libraries.\n",
    "    5. Data Science tools.\n",
    "    7. Aout arithmetic expresions.\n",
    "    8. Write codes for sum and multiplication of two numbers.\n",
    "    9. Write code for converting minutes to hours.\n",
    "    10. List of objectives.\n",
    "    11. Create Authors name.\n",
    "    12. Upload it on GitHub.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Author Name:**\n",
    "\n",
    "JASSI SHARMA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
